{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPAlign Prompt and Audio Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")\n",
    "from transformers import Wav2Vec2Processor\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# DATASET is the dataset name model trained on, e.g. ESD, MELD\n",
    "DATASET = \"ESD\" # MELD\n",
    "\n",
    "# BATCH_SIZE should smaller/equal to the category of the emotion, e.g. for RAF-DB, the category is 7\n",
    "BATCH_SIZE = 5\n",
    "EPOCH = 100\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PROJECT_PATH = os.path.join('/', *os.getcwd().split(os.sep)[:-2])\n",
    "# PROCESSED_WAV2VEC2_PATH is the path to the Wav2Vec2Processor\n",
    "PROCESSED_WAV2VEC2_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/base/wav2vec2\"\n",
    "# PRETRAIN_WAV2VEC2_PATH is the pretrained model path, e.g. EPAlign/ckpt/base/wav2vec2\n",
    "PRETRAIN_WAV2VEC2_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/base/wav2vec2\"\n",
    "# ESD_FILELIST_PATH is the path to the ESD filelist\n",
    "ESD_FILELIST_PATH = f\"{PROJECT_PATH}/EMITTS/filelist/{DATASET}\"\n",
    "# PRETRAIN_CLIP_MODEL is the pretrained CLIP model, e.g. ViT-B-32\n",
    "PRETRAIN_CLIP_MODEL = \"ViT-B/32\"\n",
    "# PRETRAIN_CLIP_MODEL_PATH is the pretrained model path, e.g. EPAlign/ckpt/base\n",
    "PRETRAIN_CLIP_MODEL_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/base\"\n",
    "# LOG_PATH is the log path, e.g. EPAlign/log\n",
    "LOG_PATH = f\"{PROJECT_PATH}/EPAlign/log\"\n",
    "# CKPT_PATH is the path to save checkpoint, e.g. EPAlign/ckpt/ESD\n",
    "CKPT_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/{DATASET}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Audio Model (consist of language model and acoustic model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CLAP were not initialized from the model checkpoint at /home/lx/project/MMTTS/EPAlign/ckpt/base/wav2vec2 and are newly initialized: ['logit_scale', 'proj', 'prompt_model.ln_final.bias', 'prompt_model.ln_final.weight', 'prompt_model.logit_scale', 'prompt_model.positional_embedding', 'prompt_model.text_projection', 'prompt_model.token_embedding.weight', 'prompt_model.transformer.resblocks.0.attn.in_proj_bias', 'prompt_model.transformer.resblocks.0.attn.in_proj_weight', 'prompt_model.transformer.resblocks.0.attn.out_proj.bias', 'prompt_model.transformer.resblocks.0.attn.out_proj.weight', 'prompt_model.transformer.resblocks.0.ln_1.bias', 'prompt_model.transformer.resblocks.0.ln_1.weight', 'prompt_model.transformer.resblocks.0.ln_2.bias', 'prompt_model.transformer.resblocks.0.ln_2.weight', 'prompt_model.transformer.resblocks.0.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.0.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.0.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.0.mlp.c_proj.weight', 'prompt_model.transformer.resblocks.1.attn.in_proj_bias', 'prompt_model.transformer.resblocks.1.attn.in_proj_weight', 'prompt_model.transformer.resblocks.1.attn.out_proj.bias', 'prompt_model.transformer.resblocks.1.attn.out_proj.weight', 'prompt_model.transformer.resblocks.1.ln_1.bias', 'prompt_model.transformer.resblocks.1.ln_1.weight', 'prompt_model.transformer.resblocks.1.ln_2.bias', 'prompt_model.transformer.resblocks.1.ln_2.weight', 'prompt_model.transformer.resblocks.1.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.1.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.1.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.1.mlp.c_proj.weight', 'prompt_model.transformer.resblocks.10.attn.in_proj_bias', 'prompt_model.transformer.resblocks.10.attn.in_proj_weight', 'prompt_model.transformer.resblocks.10.attn.out_proj.bias', 'prompt_model.transformer.resblocks.10.attn.out_proj.weight', 'prompt_model.transformer.resblocks.10.ln_1.bias', 'prompt_model.transformer.resblocks.10.ln_1.weight', 'prompt_model.transformer.resblocks.10.ln_2.bias', 'prompt_model.transformer.resblocks.10.ln_2.weight', 'prompt_model.transformer.resblocks.10.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.10.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.10.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.10.mlp.c_proj.weight', 'prompt_model.transformer.resblocks.11.attn.in_proj_bias', 'prompt_model.transformer.resblocks.11.attn.in_proj_weight', 'prompt_model.transformer.resblocks.11.attn.out_proj.bias', 'prompt_model.transformer.resblocks.11.attn.out_proj.weight', 'prompt_model.transformer.resblocks.11.ln_1.bias', 'prompt_model.transformer.resblocks.11.ln_1.weight', 'prompt_model.transformer.resblocks.11.ln_2.bias', 'prompt_model.transformer.resblocks.11.ln_2.weight', 'prompt_model.transformer.resblocks.11.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.11.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.11.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.11.mlp.c_proj.weight', 'prompt_model.transformer.resblocks.2.attn.in_proj_bias', 'prompt_model.transformer.resblocks.2.attn.in_proj_weight', 'prompt_model.transformer.resblocks.2.attn.out_proj.bias', 'prompt_model.transformer.resblocks.2.attn.out_proj.weight', 'prompt_model.transformer.resblocks.2.ln_1.bias', 'prompt_model.transformer.resblocks.2.ln_1.weight', 'prompt_model.transformer.resblocks.2.ln_2.bias', 'prompt_model.transformer.resblocks.2.ln_2.weight', 'prompt_model.transformer.resblocks.2.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.2.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.2.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.2.mlp.c_proj.weight', 'prompt_model.transformer.resblocks.3.attn.in_proj_bias', 'prompt_model.transformer.resblocks.3.attn.in_proj_weight', 'prompt_model.transformer.resblocks.3.attn.out_proj.bias', 'prompt_model.transformer.resblocks.3.attn.out_proj.weight', 'prompt_model.transformer.resblocks.3.ln_1.bias', 'prompt_model.transformer.resblocks.3.ln_1.weight', 'prompt_model.transformer.resblocks.3.ln_2.bias', 'prompt_model.transformer.resblocks.3.ln_2.weight', 'prompt_model.transformer.resblocks.3.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.3.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.3.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.3.mlp.c_proj.weight', 'prompt_model.transformer.resblocks.4.attn.in_proj_bias', 'prompt_model.transformer.resblocks.4.attn.in_proj_weight', 'prompt_model.transformer.resblocks.4.attn.out_proj.bias', 'prompt_model.transformer.resblocks.4.attn.out_proj.weight', 'prompt_model.transformer.resblocks.4.ln_1.bias', 'prompt_model.transformer.resblocks.4.ln_1.weight', 'prompt_model.transformer.resblocks.4.ln_2.bias', 'prompt_model.transformer.resblocks.4.ln_2.weight', 'prompt_model.transformer.resblocks.4.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.4.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.4.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.4.mlp.c_proj.weight', 'prompt_model.transformer.resblocks.5.attn.in_proj_bias', 'prompt_model.transformer.resblocks.5.attn.in_proj_weight', 'prompt_model.transformer.resblocks.5.attn.out_proj.bias', 'prompt_model.transformer.resblocks.5.attn.out_proj.weight', 'prompt_model.transformer.resblocks.5.ln_1.bias', 'prompt_model.transformer.resblocks.5.ln_1.weight', 'prompt_model.transformer.resblocks.5.ln_2.bias', 'prompt_model.transformer.resblocks.5.ln_2.weight', 'prompt_model.transformer.resblocks.5.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.5.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.5.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.5.mlp.c_proj.weight', 'prompt_model.transformer.resblocks.6.attn.in_proj_bias', 'prompt_model.transformer.resblocks.6.attn.in_proj_weight', 'prompt_model.transformer.resblocks.6.attn.out_proj.bias', 'prompt_model.transformer.resblocks.6.attn.out_proj.weight', 'prompt_model.transformer.resblocks.6.ln_1.bias', 'prompt_model.transformer.resblocks.6.ln_1.weight', 'prompt_model.transformer.resblocks.6.ln_2.bias', 'prompt_model.transformer.resblocks.6.ln_2.weight', 'prompt_model.transformer.resblocks.6.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.6.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.6.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.6.mlp.c_proj.weight', 'prompt_model.transformer.resblocks.7.attn.in_proj_bias', 'prompt_model.transformer.resblocks.7.attn.in_proj_weight', 'prompt_model.transformer.resblocks.7.attn.out_proj.bias', 'prompt_model.transformer.resblocks.7.attn.out_proj.weight', 'prompt_model.transformer.resblocks.7.ln_1.bias', 'prompt_model.transformer.resblocks.7.ln_1.weight', 'prompt_model.transformer.resblocks.7.ln_2.bias', 'prompt_model.transformer.resblocks.7.ln_2.weight', 'prompt_model.transformer.resblocks.7.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.7.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.7.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.7.mlp.c_proj.weight', 'prompt_model.transformer.resblocks.8.attn.in_proj_bias', 'prompt_model.transformer.resblocks.8.attn.in_proj_weight', 'prompt_model.transformer.resblocks.8.attn.out_proj.bias', 'prompt_model.transformer.resblocks.8.attn.out_proj.weight', 'prompt_model.transformer.resblocks.8.ln_1.bias', 'prompt_model.transformer.resblocks.8.ln_1.weight', 'prompt_model.transformer.resblocks.8.ln_2.bias', 'prompt_model.transformer.resblocks.8.ln_2.weight', 'prompt_model.transformer.resblocks.8.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.8.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.8.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.8.mlp.c_proj.weight', 'prompt_model.transformer.resblocks.9.attn.in_proj_bias', 'prompt_model.transformer.resblocks.9.attn.in_proj_weight', 'prompt_model.transformer.resblocks.9.attn.out_proj.bias', 'prompt_model.transformer.resblocks.9.attn.out_proj.weight', 'prompt_model.transformer.resblocks.9.ln_1.bias', 'prompt_model.transformer.resblocks.9.ln_1.weight', 'prompt_model.transformer.resblocks.9.ln_2.bias', 'prompt_model.transformer.resblocks.9.ln_2.weight', 'prompt_model.transformer.resblocks.9.mlp.c_fc.bias', 'prompt_model.transformer.resblocks.9.mlp.c_fc.weight', 'prompt_model.transformer.resblocks.9.mlp.c_proj.bias', 'prompt_model.transformer.resblocks.9.mlp.c_proj.weight', 'prompt_model.visual.class_embedding', 'prompt_model.visual.conv1.weight', 'prompt_model.visual.ln_post.bias', 'prompt_model.visual.ln_post.weight', 'prompt_model.visual.ln_pre.bias', 'prompt_model.visual.ln_pre.weight', 'prompt_model.visual.positional_embedding', 'prompt_model.visual.proj', 'prompt_model.visual.transformer.resblocks.0.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.0.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.0.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.0.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.0.ln_1.bias', 'prompt_model.visual.transformer.resblocks.0.ln_1.weight', 'prompt_model.visual.transformer.resblocks.0.ln_2.bias', 'prompt_model.visual.transformer.resblocks.0.ln_2.weight', 'prompt_model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'prompt_model.visual.transformer.resblocks.1.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.1.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.1.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.1.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.1.ln_1.bias', 'prompt_model.visual.transformer.resblocks.1.ln_1.weight', 'prompt_model.visual.transformer.resblocks.1.ln_2.bias', 'prompt_model.visual.transformer.resblocks.1.ln_2.weight', 'prompt_model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'prompt_model.visual.transformer.resblocks.10.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.10.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.10.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.10.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.10.ln_1.bias', 'prompt_model.visual.transformer.resblocks.10.ln_1.weight', 'prompt_model.visual.transformer.resblocks.10.ln_2.bias', 'prompt_model.visual.transformer.resblocks.10.ln_2.weight', 'prompt_model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'prompt_model.visual.transformer.resblocks.11.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.11.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.11.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.11.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.11.ln_1.bias', 'prompt_model.visual.transformer.resblocks.11.ln_1.weight', 'prompt_model.visual.transformer.resblocks.11.ln_2.bias', 'prompt_model.visual.transformer.resblocks.11.ln_2.weight', 'prompt_model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'prompt_model.visual.transformer.resblocks.2.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.2.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.2.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.2.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.2.ln_1.bias', 'prompt_model.visual.transformer.resblocks.2.ln_1.weight', 'prompt_model.visual.transformer.resblocks.2.ln_2.bias', 'prompt_model.visual.transformer.resblocks.2.ln_2.weight', 'prompt_model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'prompt_model.visual.transformer.resblocks.3.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.3.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.3.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.3.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.3.ln_1.bias', 'prompt_model.visual.transformer.resblocks.3.ln_1.weight', 'prompt_model.visual.transformer.resblocks.3.ln_2.bias', 'prompt_model.visual.transformer.resblocks.3.ln_2.weight', 'prompt_model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'prompt_model.visual.transformer.resblocks.4.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.4.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.4.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.4.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.4.ln_1.bias', 'prompt_model.visual.transformer.resblocks.4.ln_1.weight', 'prompt_model.visual.transformer.resblocks.4.ln_2.bias', 'prompt_model.visual.transformer.resblocks.4.ln_2.weight', 'prompt_model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'prompt_model.visual.transformer.resblocks.5.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.5.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.5.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.5.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.5.ln_1.bias', 'prompt_model.visual.transformer.resblocks.5.ln_1.weight', 'prompt_model.visual.transformer.resblocks.5.ln_2.bias', 'prompt_model.visual.transformer.resblocks.5.ln_2.weight', 'prompt_model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'prompt_model.visual.transformer.resblocks.6.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.6.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.6.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.6.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.6.ln_1.bias', 'prompt_model.visual.transformer.resblocks.6.ln_1.weight', 'prompt_model.visual.transformer.resblocks.6.ln_2.bias', 'prompt_model.visual.transformer.resblocks.6.ln_2.weight', 'prompt_model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'prompt_model.visual.transformer.resblocks.7.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.7.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.7.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.7.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.7.ln_1.bias', 'prompt_model.visual.transformer.resblocks.7.ln_1.weight', 'prompt_model.visual.transformer.resblocks.7.ln_2.bias', 'prompt_model.visual.transformer.resblocks.7.ln_2.weight', 'prompt_model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'prompt_model.visual.transformer.resblocks.8.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.8.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.8.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.8.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.8.ln_1.bias', 'prompt_model.visual.transformer.resblocks.8.ln_1.weight', 'prompt_model.visual.transformer.resblocks.8.ln_2.bias', 'prompt_model.visual.transformer.resblocks.8.ln_2.weight', 'prompt_model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'prompt_model.visual.transformer.resblocks.9.attn.in_proj_bias', 'prompt_model.visual.transformer.resblocks.9.attn.in_proj_weight', 'prompt_model.visual.transformer.resblocks.9.attn.out_proj.bias', 'prompt_model.visual.transformer.resblocks.9.attn.out_proj.weight', 'prompt_model.visual.transformer.resblocks.9.ln_1.bias', 'prompt_model.visual.transformer.resblocks.9.ln_1.weight', 'prompt_model.visual.transformer.resblocks.9.ln_2.bias', 'prompt_model.visual.transformer.resblocks.9.ln_2.weight', 'prompt_model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'prompt_model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'prompt_model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'prompt_model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class CLAP(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config, prompt_pretrain_model, prompt_pretrain_model_path):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.init_weights()\n",
    "        width = 1024\n",
    "        scale = width ** -0.5\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, 512))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.prompt_model, self.prompt_processor = clip.load(prompt_pretrain_model, jit=False, download_root=prompt_pretrain_model_path)\n",
    "        self.prompt_model.to(device)\n",
    "    def forward(self, wavs, prompts):\n",
    "        audio_features = torch.tensor([]).to(device)\n",
    "        for wav in wavs:\n",
    "            audio_feature = self.wav2vec2(wav)\n",
    "            audio_feature = audio_feature[0]\n",
    "            audio_feature = torch.mean(audio_feature, dim=1)\n",
    "            audio_features = torch.cat((audio_features, audio_feature), dim=0)\n",
    "        audio_features = audio_features @ self.proj\n",
    "\n",
    "        prompt_features = clip.tokenize(prompts).to(device)\n",
    "        prompt_features = self.prompt_model.encode_text(prompt_features)\n",
    "        # normalized features\n",
    "        audio_features = audio_features / audio_features.norm(dim=1, keepdim=True)\n",
    "        prompt_features = prompt_features / prompt_features.norm(dim=1, keepdim=True)\n",
    "        audio_features = audio_features.float()\n",
    "        prompt_features = prompt_features.float()\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp().float()\n",
    "        logits_per_audio = logit_scale * audio_features @ prompt_features.t()\n",
    "        logits_per_text = logits_per_audio.t()\n",
    "        return logits_per_audio, logits_per_text\n",
    "\n",
    "model = CLAP.from_pretrained(PRETRAIN_WAV2VEC2_PATH, prompt_pretrain_model=PRETRAIN_CLIP_MODEL, prompt_pretrain_model_path=PRETRAIN_CLIP_MODEL_PATH).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Wav2Vec2 Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(PROCESSED_WAV2VEC2_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESDDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 datalist=\"path/to/datalist\",\n",
    "                 preprocess=None):\n",
    "        self.datalist = datalist\n",
    "        self.preprocess = preprocess\n",
    "        self.data = self.load_data()\n",
    "        self.text2label = {\n",
    "            \"angry\": 1,\n",
    "            \"happy\": 2,\n",
    "            \"neutral\": 3,\n",
    "            \"sad\": 4,\n",
    "            \"surprise\": 5,\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        wav_path = data[0]\n",
    "        wav, _ = librosa.load(wav_path, sr=16000)\n",
    "        # audio = torch.from_numpy(wav).float()\n",
    "        if self.preprocess is not None:\n",
    "            audio = self.preprocess(wav, sampling_rate=16000)\n",
    "            audio = audio[\"input_values\"][0]\n",
    "            audio = audio.reshape(1, -1)\n",
    "            audio = torch.from_numpy(audio).to(device).float()\n",
    "\n",
    "        prompt_feature_path = data[3]\n",
    "        emotiontag = prompt_feature_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        prompt = f\"A person speaking with a feeling of {emotiontag}\"\n",
    "        label = self.text2label[emotiontag]\n",
    "    \n",
    "        return audio, prompt, label\n",
    "    \n",
    "    def load_data(self):\n",
    "        with open(self.datalist, encoding='utf-8') as f:\n",
    "            data = [line.strip().split(\"|\") for line in f]\n",
    "        return data\n",
    "    \n",
    "if DATASET == \"ESD\":\n",
    "    train_dataset = ESDDataset(datalist=f'{ESD_FILELIST_PATH}/esd_en_audio_sid_text_efeature_train_filelist.txt', preprocess=processor)\n",
    "    val_dataset = ESDDataset(datalist=f'{ESD_FILELIST_PATH}/esd_en_audio_sid_text_efeature_val_filelist.txt', preprocess=processor)\n",
    "    test_dataset = ESDDataset(datalist=f'{ESD_FILELIST_PATH}/esd_en_audio_sid_text_efeature_test_filelist.txt', preprocess=processor)\n",
    "\n",
    "assert len(train_dataset) == 14_000\n",
    "assert len(val_dataset) == 1_750\n",
    "assert len(test_dataset) == 1_750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Batch Sample (ensures no same class per batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.labels_set = list(set(self.labels.numpy()))\n",
    "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dataset = len(self.labels)\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < self.n_dataset:\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_dataset // self.batch_size\n",
    "\n",
    "def collate_fn(batch):\n",
    "    audios = [sample[0] for sample in batch]\n",
    "    # 其他特征不变\n",
    "    prompts = [sample[1] for sample in batch]\n",
    "    labels = [sample[2] for sample in batch]\n",
    "    labels = torch.tensor(labels).to(device)\n",
    "\n",
    "    return audios, prompts, labels\n",
    "\n",
    "train_labels = torch.tensor([item[2] for item in train_dataset])\n",
    "train_sampler = BalancedBatchSampler(train_labels, BATCH_SIZE, 1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler, collate_fn=collate_fn)\n",
    "\n",
    "test_labels = torch.tensor([item[2] for item in test_dataset])\n",
    "test_sampler = BalancedBatchSampler(test_labels, BATCH_SIZE, 1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_sampler=test_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "loss_audio = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "optimizer = optim.Adam([model.proj, model.logit_scale], lr=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*EPOCH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 15:16:37,716 - INFO - finetune start...\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "file_handler = logging.FileHandler(f\"{LOG_PATH}/log_prompt_audio_{DATASET}.txt\")\n",
    "\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "log = logging.getLogger('')\n",
    "log.addHandler(file_handler)\n",
    "log.info('finetune start...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch 0, best test loss 100000.0 after epoch -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, tr_loss 1.4612626329665952, te_loss 1.3376362760975573\n",
      "running epoch 1, best test loss 1.3376362760975573 after epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m audios, prompts, _ \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 14\u001b[0m logits_per_audio, logits_per_text \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(BATCH_SIZE)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m (loss_audio(logits_per_audio,ground_truth) \u001b[38;5;241m+\u001b[39m loss_txt(logits_per_text,ground_truth))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[59], line 23\u001b[0m, in \u001b[0;36mCLAP.forward\u001b[0;34m(self, wavs, prompts)\u001b[0m\n\u001b[1;32m     20\u001b[0m audio_features \u001b[38;5;241m=\u001b[39m audio_features \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj\n\u001b[1;32m     22\u001b[0m prompt_features \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mtokenize(prompts)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m prompt_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# normalized features\u001b[39;00m\n\u001b[1;32m     25\u001b[0m audio_features \u001b[38;5;241m=\u001b[39m audio_features \u001b[38;5;241m/\u001b[39m audio_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/clip/model.py:348\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    346\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    347\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    350\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(x)\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/clip/model.py:203\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/clip/model.py:191\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    190\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[0;32m--> 191\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/MMTTS/lib/python3.12/site-packages/torch/nn/modules/module.py:1535\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1535\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_te_loss = 1e5\n",
    "best_ep = -1\n",
    "for epoch in range(EPOCH):\n",
    "    print(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n",
    "    step = 0\n",
    "    tr_loss = 0\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader, leave=False)\n",
    "    for batch in pbar:\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        audios, prompts, _ = batch\n",
    "        logits_per_audio, logits_per_text = model(audios, prompts)\n",
    "        ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "        total_loss = (loss_audio(logits_per_audio,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        tr_loss += total_loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        pbar.set_description(f\"train batchCE: {total_loss.item()}\", refresh=True)\n",
    "    tr_loss /= step\n",
    "    \n",
    "    step = 0\n",
    "    te_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_pbar = tqdm(test_dataloader, leave=False)\n",
    "        for batch in test_pbar:\n",
    "            step += 1\n",
    "            audios, texts, _ = batch\n",
    "            logits_per_audio, logits_per_text = model(audios, texts)\n",
    "            ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "            total_loss = (loss_audio(logits_per_audio,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            te_loss += total_loss.item()\n",
    "            test_pbar.set_description(f\"test batchCE: {total_loss.item()}\", refresh=True)\n",
    "        te_loss /= step\n",
    "        \n",
    "    if te_loss < best_te_loss:\n",
    "        best_te_loss = te_loss\n",
    "        best_ep = epoch\n",
    "        torch.save(model.state_dict(), f\"{CKPT_PATH}/best_model_proj_logit.pt\")\n",
    "        torch.save(model.prompt_model.state_dict(), f\"{CKPT_PATH}/best_model.pt\")\n",
    "    print(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")\n",
    "    # torch.save(model.state_dict(), f\"{CKPT_PATH}/ESD_ft_proj_logit_{epoch}_model.pt\")\n",
    "    # torch.save(model.prompt_model.state_dict(), f'{CKPT_PATH}/model_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMTTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
